# Local LLM Configuration

# Choose your local LLM backend
LOCAL_LLM_BACKEND=gpt4all  # Options: gpt4all, ollama, llama-cpp, transformers

# GPT4All Settings
GPT4ALL_MODEL=mistral-7b-openorca.Q4_0.gguf  # Recommended: fast and accurate
# Other options:
# - orca-mini-3b.ggmlv3.q4_0.bin (smallest, fastest)
# - wizardlm-13b-v1.2.Q4_0.gguf (larger, more capable)
# - gpt4all-falcon-q4_0.gguf (good balance)

# Ollama Settings (if using Ollama)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=mistral:7b  # Options: llama2, codellama, mistral, phi

# Model Performance Settings
LOCAL_MODEL_MAX_TOKENS=500  # Reduce for faster responses
LOCAL_MODEL_CONTEXT_LENGTH=2048  # Context window size
LOCAL_MODEL_THREADS=4  # CPU threads (adjust based on your CPU)
LOCAL_MODEL_GPU_LAYERS=0  # Number of layers on GPU (0 for CPU only)

# Performance Optimization
ENABLE_MODEL_CACHING=true
MODEL_CACHE_SIZE=100  # Number of cached responses
USE_QUANTIZED_MODELS=true  # Use Q4 or Q5 quantization

# Privacy Settings
KEEP_DATA_LOCAL=true  # Never send data to cloud
DISABLE_TELEMETRY=true

# Hardware Optimization
# CPU: Set threads to your CPU core count
# GPU: Set gpu_layers to 20-30 for NVIDIA GPUs with 8GB+ VRAM
# RAM: Recommended 8GB+ for 7B models, 16GB+ for 13B models

# All other settings from original .env
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/agentic_ai
REDIS_URL=redis://localhost:6379
SECRET_KEY=your-secret-key-here

# Optional: Keep API keys for fallback
OPENAI_API_KEY=  # Leave empty to force local mode
ANTHROPIC_API_KEY=

# Email Service
EMAIL_SERVICE_API_KEY=
IMAP_HOST=imap.gmail.com
IMAP_PORT=993

# Calendar
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=

# Stripe
STRIPE_SECRET_KEY=
STRIPE_WEBHOOK_SECRET=
